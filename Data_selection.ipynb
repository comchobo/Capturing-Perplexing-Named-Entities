{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used dataset titled \"Korean-English Parallel Corpus of Specialized Domains\" \n",
    "\n",
    "https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=111\n",
    "\n",
    "However, it has a limited access to nationals (Korean).\n",
    "\n",
    "The structure of dataset is as follows : \n",
    "\n",
    "> data\n",
    ">> Training\n",
    ">>> ko2en_training_json.zip\n",
    ">>>> ko2en_medical_1_training.json (and so on)\n",
    "\n",
    ">> Validation\n",
    ">>> ko2en_validation_json.zip\n",
    ">>>> ko2en_medical_1_validation.json (and so on)\n",
    " \n",
    "Each domain data consists following key-values (I added translation on each key):\n",
    "\n",
    "&nbsp; \"sid\": 3\\\n",
    "&nbsp; \"분야(domain)\": \"의료/보건\",\\\n",
    "&nbsp; \"한국어(ko)\": \"쿠퍼만지수의 대표 증상인 안면홍조, 손발저림, 신경과민, 우울증, 가슴 두근거림, 근관절통, 피로 등에서 뛰어난 효과를 나타냈다고 회사 측은 설명했다.\",\\\n",
    "&nbsp; \"영어(en)\": \"The company explained that it had an excellent effect on the representative symptoms of Kupperman's index such as hot flashes, numbness in the hands and feet, nervousness, depression, palpitations in the chest, muscle joint pain, and fatigue.\",\\\n",
    "&nbsp; \"한국어_어절수(ko_word_count)\": 18,\\\n",
    "&nbsp; \"영어_단어수(en_word_count)\": 37,\\\n",
    "&nbsp; \"길이_분류(length_category)\": 3,\\\n",
    "&nbsp; \"난이도(difficulty)\": \"중\",\\\n",
    "&nbsp; \"수행기관(work_done_by)\": \"에버트란\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "debug = True # decide debug mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Environments are less likely to make dependency problem, but torch>=2.0.0 is needed to use `optimum.bettertransformer`\n",
    "\n",
    "Requirements:\n",
    "\n",
    "!pip install transformers, datasets, tqdm, optimum, comet, sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "`input_path = './domain_datasets'` should be split into train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import set_seed\n",
    "from torch import device as set_device\n",
    "\n",
    "set_seed(42) # seed 40, 41, 42 were used\n",
    "batch_size = 8 # available bsz in gtx 3090 (8~10 for VRAM 16GB)\n",
    "device = set_device('cuda')\n",
    "pretrained_model_path = 'facebook/nllb-200-distilled-1.3B'\n",
    "\n",
    "input_path = './domain_datasets'\n",
    "output_path = './pruned_datasets'\n",
    "\n",
    "try:\n",
    "    os.mkdir(f'{output_path}')\n",
    "    os.mkdir(f'{output_path}/travel')\n",
    "    os.mkdir(f'{output_path}/sports')\n",
    "    os.mkdir(f'{output_path}/law')\n",
    "    os.mkdir(f'{output_path}/medical')\n",
    "except Exception :\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pruning_fuctions import el2n_algorithm, entropy_algorithm, get_offset_mapping\n",
    "from transformers import M2M100ForConditionalGeneration, AutoTokenizer, DataCollatorWithPadding, AutoModelForTokenClassification, AutoModel, TokenClassificationPipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from datasets import Dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define model & tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = M2M100ForConditionalGeneration.from_pretrained(pretrained_model_path).eval()\n",
    "pretrained_model = BetterTransformer.transform(pretrained_model)\n",
    "pretrained_model.to(device)\n",
    "NLLB_tokenizer_src  = AutoTokenizer.from_pretrained(pretrained_model_path, src_lang='kor_Hang')\n",
    "NLLB_tokenizer_tgt = AutoTokenizer.from_pretrained(pretrained_model_path, src_lang='eng_Latn')\n",
    "\n",
    "# mean + 3 sigma is set for efficient handling\n",
    "def tokenize_source(row):\n",
    "    return NLLB_tokenizer_src(row['ko'], truncation=True, max_length=72, return_offsets_mapping=True)\n",
    "\n",
    "def tokenize_target(row):\n",
    "    return NLLB_tokenizer_tgt(row['en'], truncation=True, max_length=144, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define detailed tokenizing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_dataset(which_domain):\n",
    "    train_dataset = load_from_disk(f'{input_path}/{which_domain}')['train']\n",
    "    if debug is True : train_dataset = train_dataset.shard(500,0)\n",
    "    tokenized_train_dataset = train_dataset.map(tokenize_source, batched=True)\n",
    "    tokenized_train_dataset = tokenized_train_dataset.map(lambda row : {'len': len(row['input_ids'])})\n",
    "\n",
    "    # for efficient pruning, we sorted data by length\n",
    "    sorted_train_dataset = tokenized_train_dataset.sort('len', reverse=True)\n",
    "\n",
    "    sorted_tokenized_answer = sorted_train_dataset.map(tokenize_target, batched=True)\n",
    "    src_sentences = sorted_train_dataset['ko']\n",
    "    tgt_sentences = sorted_train_dataset['en']\n",
    "    sorted_tokenized_answer = sorted_tokenized_answer.remove_columns(['ko', 'en', 'attention_mask','len'])\n",
    "    sorted_input_dataset = sorted_train_dataset.remove_columns(['ko', 'en', 'offset_mapping', 'len'])\n",
    "\n",
    "    return sorted_input_dataset, sorted_tokenized_answer, src_sentences, tgt_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define collator & dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_dataloader(input_dataset, tokenizer):\n",
    "    collator = DataCollatorWithPadding(tokenizer, return_tensors='pt')\n",
    "    dataloader = DataLoader(input_dataset, batch_size, collate_fn=collator, pin_memory=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "prune methods using NER first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def translate(model, dataloader, sorted_tokenized_answer, tgt_lang_id):\n",
    "    n = 0\n",
    "    translated_sentences = []\n",
    "    el2n_list = []\n",
    "    entropy_list = []\n",
    "    outputs_offset_mappings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(dataloader):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            batch_len = inputs['input_ids'].shape[0]\n",
    "\n",
    "            translated_tokens = model.generate(\n",
    "                **inputs, forced_bos_token_id=tgt_lang_id, max_length=144,\n",
    "                return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "            # offset mappings of translated tokens\n",
    "            res_decoded_originals = [NLLB_tokenizer_tgt.convert_ids_to_tokens(res_token)\n",
    "                                     for res_token in translated_tokens.sequences]\n",
    "            outputs_offset_mappings += [get_offset_mapping(res_decoded_original)\n",
    "                                       for res_decoded_original in res_decoded_originals]\n",
    "\n",
    "            # logit score for logit method\n",
    "            scores = torch.stack([x for x in translated_tokens.scores]).movedim(0, 1)\n",
    "            tokenized_answer_split = sorted_tokenized_answer[n:n + batch_len]['input_ids']\n",
    "            answer_ids_onehot = [F.one_hot(torch.tensor(x), num_classes=scores.shape[2]).to(device) for x in\n",
    "                                 tokenized_answer_split]\n",
    "\n",
    "            el2n_list+=el2n_algorithm(scores, translated_tokens.sequences, answer_ids_onehot)\n",
    "            entropy_list+=entropy_algorithm(scores, translated_tokens.sequences)\n",
    "\n",
    "            # 'translated' outputs\n",
    "            translated_sentences += NLLB_tokenizer_tgt.batch_decode(translated_tokens.sequences, skip_special_tokens=True)\n",
    "            n += batch_len\n",
    "\n",
    "    return translated_sentences, el2n_list, entropy_list, outputs_offset_mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Do translate\n",
    "and save easily made metrics\n",
    "**translated_dict** works as a global translated dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[AYou're using a NllbTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\jish1\\PycharmProjects\\COLING-paper-project\\venv\\lib\\site-packages\\optimum\\bettertransformer\\models\\encoder_models.py:682: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ..\\aten\\src\\ATen\\NestedTensorImpl.cpp:179.)\n",
      "  hidden_states = torch._nested_tensor_from_mask(hidden_states, ~attention_mask)\n",
      "\n",
      "  2%|▏         | 1/50 [00:03<03:09,  3.87s/it]\u001b[A\n",
      "  4%|▍         | 2/50 [00:06<02:20,  2.92s/it]\u001b[A\n",
      "  6%|▌         | 3/50 [00:07<01:52,  2.40s/it]\u001b[A\n",
      "  8%|▊         | 4/50 [00:09<01:36,  2.09s/it]\u001b[A\n",
      " 10%|█         | 5/50 [00:11<01:29,  1.99s/it]\u001b[A\n",
      " 12%|█▏        | 6/50 [00:12<01:21,  1.85s/it]\u001b[A\n",
      " 14%|█▍        | 7/50 [00:14<01:19,  1.84s/it]\u001b[A\n",
      " 16%|█▌        | 8/50 [00:16<01:12,  1.74s/it]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:17<01:07,  1.65s/it]\u001b[A\n",
      " 20%|██        | 10/50 [00:19<01:03,  1.60s/it]\u001b[A\n",
      " 22%|██▏       | 11/50 [00:20<01:01,  1.58s/it]\u001b[A\n",
      " 24%|██▍       | 12/50 [00:22<01:00,  1.59s/it]\u001b[A\n",
      " 26%|██▌       | 13/50 [00:23<00:56,  1.53s/it]\u001b[A\n",
      " 28%|██▊       | 14/50 [00:25<00:54,  1.52s/it]\u001b[A\n",
      " 30%|███       | 15/50 [00:26<00:54,  1.57s/it]\u001b[A\n",
      " 32%|███▏      | 16/50 [00:28<00:50,  1.50s/it]\u001b[A\n",
      " 34%|███▍      | 17/50 [00:29<00:46,  1.42s/it]\u001b[A\n",
      " 36%|███▌      | 18/50 [00:30<00:45,  1.43s/it]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:32<00:43,  1.41s/it]\u001b[A\n",
      " 40%|████      | 20/50 [00:33<00:40,  1.36s/it]\u001b[A\n",
      " 42%|████▏     | 21/50 [00:34<00:38,  1.32s/it]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:36<00:37,  1.33s/it]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:37<00:35,  1.33s/it]\u001b[A\n",
      " 48%|████▊     | 24/50 [00:38<00:34,  1.34s/it]\u001b[A\n",
      " 50%|█████     | 25/50 [00:39<00:31,  1.28s/it]\u001b[A\n",
      " 52%|█████▏    | 26/50 [00:41<00:29,  1.24s/it]\u001b[A\n",
      " 54%|█████▍    | 27/50 [00:42<00:28,  1.23s/it]\u001b[A\n",
      " 56%|█████▌    | 28/50 [00:43<00:27,  1.27s/it]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:44<00:25,  1.21s/it]\u001b[A\n",
      " 60%|██████    | 30/50 [00:45<00:23,  1.18s/it]\u001b[A\n",
      " 62%|██████▏   | 31/50 [00:46<00:21,  1.12s/it]\u001b[A\n",
      " 64%|██████▍   | 32/50 [00:47<00:19,  1.07s/it]\u001b[A\n",
      " 66%|██████▌   | 33/50 [00:48<00:18,  1.06s/it]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:49<00:16,  1.01s/it]\u001b[A\n",
      " 70%|███████   | 35/50 [00:50<00:15,  1.03s/it]\u001b[A\n",
      " 72%|███████▏  | 36/50 [00:51<00:15,  1.08s/it]\u001b[A\n",
      " 74%|███████▍  | 37/50 [00:52<00:13,  1.03s/it]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:53<00:11,  1.00it/s]\u001b[A\n",
      " 78%|███████▊  | 39/50 [00:54<00:10,  1.00it/s]\u001b[A\n",
      " 80%|████████  | 40/50 [00:55<00:09,  1.03it/s]\u001b[A\n",
      " 82%|████████▏ | 41/50 [00:56<00:08,  1.06it/s]\u001b[A\n",
      " 84%|████████▍ | 42/50 [00:57<00:07,  1.10it/s]\u001b[A\n",
      " 86%|████████▌ | 43/50 [00:58<00:06,  1.13it/s]\u001b[A\n",
      " 88%|████████▊ | 44/50 [00:58<00:05,  1.20it/s]\u001b[A\n",
      " 90%|█████████ | 45/50 [00:59<00:04,  1.21it/s]\u001b[A\n",
      " 92%|█████████▏| 46/50 [01:00<00:03,  1.26it/s]\u001b[A\n",
      " 94%|█████████▍| 47/50 [01:01<00:02,  1.27it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [01:01<00:01,  1.41it/s]\u001b[A\n",
      " 98%|█████████▊| 49/50 [01:02<00:00,  1.42it/s]\u001b[A\n",
      "100%|██████████| 50/50 [01:03<00:00,  1.27s/it]\u001b[A\n",
      " 25%|██▌       | 1/4 [01:04<03:12, 64.29s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3dad532991471cbe2677806697c7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60ef0ae76054cc6803f60a8cd522f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8bf1b01d8948c68878908ad76bc8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▎         | 1/40 [00:01<01:06,  1.71s/it]\u001b[A\n",
      "  5%|▌         | 2/40 [00:03<01:02,  1.66s/it]\u001b[A\n",
      "  8%|▊         | 3/40 [00:06<01:27,  2.37s/it]\u001b[A\n",
      " 10%|█         | 4/40 [00:08<01:15,  2.11s/it]\u001b[A\n",
      " 12%|█▎        | 5/40 [00:09<01:06,  1.90s/it]\u001b[A\n",
      " 15%|█▌        | 6/40 [00:11<01:00,  1.78s/it]\u001b[A\n",
      " 18%|█▊        | 7/40 [00:12<00:54,  1.67s/it]\u001b[A\n",
      " 20%|██        | 8/40 [00:14<00:51,  1.60s/it]\u001b[A\n",
      " 22%|██▎       | 9/40 [00:17<01:04,  2.09s/it]\u001b[A\n",
      " 25%|██▌       | 10/40 [00:18<00:55,  1.86s/it]\u001b[A\n",
      " 28%|██▊       | 11/40 [00:20<00:49,  1.70s/it]\u001b[A\n",
      " 30%|███       | 12/40 [00:21<00:44,  1.58s/it]\u001b[A\n",
      " 32%|███▎      | 13/40 [00:22<00:40,  1.52s/it]\u001b[A\n",
      " 35%|███▌      | 14/40 [00:23<00:36,  1.42s/it]\u001b[A\n",
      " 38%|███▊      | 15/40 [00:25<00:34,  1.37s/it]\u001b[A\n",
      " 40%|████      | 16/40 [00:26<00:33,  1.38s/it]\u001b[A\n",
      " 42%|████▎     | 17/40 [00:27<00:29,  1.29s/it]\u001b[A\n",
      " 45%|████▌     | 18/40 [00:28<00:27,  1.24s/it]\u001b[A\n",
      " 48%|████▊     | 19/40 [00:29<00:24,  1.18s/it]\u001b[A\n",
      " 50%|█████     | 20/40 [00:31<00:23,  1.19s/it]\u001b[A\n",
      " 52%|█████▎    | 21/40 [00:32<00:22,  1.20s/it]\u001b[A\n",
      " 55%|█████▌    | 22/40 [00:33<00:20,  1.12s/it]\u001b[A\n",
      " 57%|█████▊    | 23/40 [00:34<00:18,  1.08s/it]\u001b[A\n",
      " 60%|██████    | 24/40 [00:35<00:17,  1.08s/it]\u001b[A\n",
      " 62%|██████▎   | 25/40 [00:36<00:15,  1.03s/it]\u001b[A\n",
      " 65%|██████▌   | 26/40 [00:37<00:13,  1.00it/s]\u001b[A\n",
      " 68%|██████▊   | 27/40 [00:38<00:12,  1.00it/s]\u001b[A\n",
      " 70%|███████   | 28/40 [00:39<00:11,  1.04it/s]\u001b[A\n",
      " 72%|███████▎  | 29/40 [00:39<00:10,  1.08it/s]\u001b[A\n",
      " 75%|███████▌  | 30/40 [00:40<00:09,  1.09it/s]\u001b[A\n",
      " 78%|███████▊  | 31/40 [00:41<00:08,  1.11it/s]\u001b[A\n",
      " 80%|████████  | 32/40 [00:42<00:06,  1.17it/s]\u001b[A\n",
      " 82%|████████▎ | 33/40 [00:43<00:05,  1.22it/s]\u001b[A\n",
      " 85%|████████▌ | 34/40 [00:43<00:04,  1.23it/s]\u001b[A\n",
      " 88%|████████▊ | 35/40 [00:44<00:04,  1.21it/s]\u001b[A\n",
      " 90%|█████████ | 36/40 [00:45<00:03,  1.28it/s]\u001b[A\n",
      " 92%|█████████▎| 37/40 [00:46<00:02,  1.28it/s]\u001b[A\n",
      " 95%|█████████▌| 38/40 [00:46<00:01,  1.34it/s]\u001b[A\n",
      " 98%|█████████▊| 39/40 [00:47<00:00,  1.33it/s]\u001b[A\n",
      "100%|██████████| 40/40 [00:48<00:00,  1.21s/it]\u001b[A\n",
      " 50%|█████     | 2/4 [01:53<01:50, 55.36s/it]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 1/30 [00:03<01:36,  3.34s/it]\u001b[A\n",
      "  7%|▋         | 2/30 [00:06<01:32,  3.31s/it]\u001b[A\n",
      " 10%|█         | 3/30 [00:08<01:11,  2.64s/it]\u001b[A\n",
      " 13%|█▎        | 4/30 [00:10<00:57,  2.21s/it]\u001b[A\n",
      " 17%|█▋        | 5/30 [00:11<00:50,  2.03s/it]\u001b[A\n",
      " 20%|██        | 6/30 [00:13<00:43,  1.83s/it]\u001b[A\n",
      " 23%|██▎       | 7/30 [00:14<00:39,  1.70s/it]\u001b[A\n",
      " 27%|██▋       | 8/30 [00:16<00:35,  1.61s/it]\u001b[A\n",
      " 30%|███       | 9/30 [00:17<00:32,  1.54s/it]\u001b[A\n",
      " 33%|███▎      | 10/30 [00:18<00:30,  1.54s/it]\u001b[A\n",
      " 37%|███▋      | 11/30 [00:20<00:26,  1.42s/it]\u001b[A\n",
      " 40%|████      | 12/30 [00:21<00:24,  1.34s/it]\u001b[A\n",
      " 43%|████▎     | 13/30 [00:22<00:21,  1.29s/it]\u001b[A\n",
      " 47%|████▋     | 14/30 [00:23<00:20,  1.29s/it]\u001b[A\n",
      " 50%|█████     | 15/30 [00:25<00:19,  1.30s/it]\u001b[A\n",
      " 53%|█████▎    | 16/30 [00:26<00:16,  1.20s/it]\u001b[A\n",
      " 57%|█████▋    | 17/30 [00:27<00:15,  1.21s/it]\u001b[A\n",
      " 60%|██████    | 18/30 [00:28<00:13,  1.14s/it]\u001b[A\n",
      " 63%|██████▎   | 19/30 [00:29<00:12,  1.12s/it]\u001b[A\n",
      " 67%|██████▋   | 20/30 [00:30<00:11,  1.13s/it]\u001b[A\n",
      " 70%|███████   | 21/30 [00:31<00:09,  1.07s/it]\u001b[A\n",
      " 73%|███████▎  | 22/30 [00:32<00:08,  1.05s/it]\u001b[A\n",
      " 77%|███████▋  | 23/30 [00:33<00:06,  1.01it/s]\u001b[A\n",
      " 80%|████████  | 24/30 [00:34<00:05,  1.03it/s]\u001b[A\n",
      " 83%|████████▎ | 25/30 [00:35<00:04,  1.03it/s]\u001b[A\n",
      " 87%|████████▋ | 26/30 [00:36<00:03,  1.06it/s]\u001b[A\n",
      " 90%|█████████ | 27/30 [00:36<00:02,  1.13it/s]\u001b[A\n",
      " 93%|█████████▎| 28/30 [00:37<00:01,  1.19it/s]\u001b[A\n",
      " 97%|█████████▋| 29/30 [00:38<00:00,  1.27it/s]\u001b[A\n",
      "100%|██████████| 30/30 [00:38<00:00,  1.29s/it]\u001b[A\n",
      " 75%|███████▌  | 3/4 [02:32<00:48, 48.11s/it]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▎         | 1/40 [00:02<01:24,  2.16s/it]\u001b[A\n",
      "  5%|▌         | 2/40 [00:04<01:15,  1.98s/it]\u001b[A\n",
      "  8%|▊         | 3/40 [00:05<01:07,  1.81s/it]\u001b[A\n",
      " 10%|█         | 4/40 [00:08<01:14,  2.06s/it]\u001b[A\n",
      " 12%|█▎        | 5/40 [00:11<01:26,  2.47s/it]\u001b[A\n",
      " 15%|█▌        | 6/40 [00:12<01:13,  2.17s/it]\u001b[A\n",
      " 18%|█▊        | 7/40 [00:14<01:02,  1.90s/it]\u001b[A\n",
      " 20%|██        | 8/40 [00:15<00:54,  1.72s/it]\u001b[A\n",
      " 22%|██▎       | 9/40 [00:16<00:50,  1.62s/it]\u001b[A\n",
      " 25%|██▌       | 10/40 [00:18<00:46,  1.55s/it]\u001b[A\n",
      " 28%|██▊       | 11/40 [00:19<00:42,  1.48s/it]\u001b[A\n",
      " 30%|███       | 12/40 [00:20<00:39,  1.40s/it]\u001b[A\n",
      " 32%|███▎      | 13/40 [00:22<00:38,  1.43s/it]\u001b[A\n",
      " 35%|███▌      | 14/40 [00:23<00:34,  1.33s/it]\u001b[A\n",
      " 38%|███▊      | 15/40 [00:24<00:31,  1.25s/it]\u001b[A\n",
      " 40%|████      | 16/40 [00:25<00:29,  1.21s/it]\u001b[A\n",
      " 42%|████▎     | 17/40 [00:26<00:28,  1.22s/it]\u001b[A\n",
      " 45%|████▌     | 18/40 [00:27<00:25,  1.17s/it]\u001b[A\n",
      " 48%|████▊     | 19/40 [00:29<00:24,  1.16s/it]\u001b[A\n",
      " 50%|█████     | 20/40 [00:30<00:23,  1.16s/it]\u001b[A\n",
      " 52%|█████▎    | 21/40 [00:31<00:21,  1.14s/it]\u001b[A\n",
      " 55%|█████▌    | 22/40 [00:32<00:19,  1.10s/it]\u001b[A\n",
      " 57%|█████▊    | 23/40 [00:33<00:18,  1.08s/it]\u001b[A\n",
      " 60%|██████    | 24/40 [00:34<00:16,  1.03s/it]\u001b[A\n",
      " 62%|██████▎   | 25/40 [00:35<00:15,  1.04s/it]\u001b[A\n",
      " 65%|██████▌   | 26/40 [00:36<00:14,  1.02s/it]\u001b[A\n",
      " 68%|██████▊   | 27/40 [00:37<00:12,  1.00it/s]\u001b[A\n",
      " 70%|███████   | 28/40 [00:38<00:11,  1.03it/s]\u001b[A\n",
      " 72%|███████▎  | 29/40 [00:39<00:10,  1.07it/s]\u001b[A\n",
      " 75%|███████▌  | 30/40 [00:39<00:08,  1.12it/s]\u001b[A\n",
      " 78%|███████▊  | 31/40 [00:40<00:07,  1.13it/s]\u001b[A\n",
      " 80%|████████  | 32/40 [00:41<00:06,  1.15it/s]\u001b[A\n",
      " 82%|████████▎ | 33/40 [00:42<00:05,  1.20it/s]\u001b[A\n",
      " 85%|████████▌ | 34/40 [00:42<00:04,  1.26it/s]\u001b[A\n",
      " 88%|████████▊ | 35/40 [00:43<00:03,  1.30it/s]\u001b[A\n",
      " 90%|█████████ | 36/40 [00:44<00:02,  1.37it/s]\u001b[A\n",
      " 92%|█████████▎| 37/40 [00:44<00:02,  1.43it/s]\u001b[A\n",
      " 95%|█████████▌| 38/40 [00:45<00:01,  1.46it/s]\u001b[A\n",
      " 98%|█████████▊| 39/40 [00:46<00:00,  1.54it/s]\u001b[A\n",
      "100%|██████████| 40/40 [00:46<00:00,  1.17s/it]\u001b[A\n",
      "100%|██████████| 4/4 [03:20<00:00, 50.05s/it]\n"
     ]
    }
   ],
   "source": [
    "domains = ['medical','sports','law','travel']\n",
    "translated_dict = {x:{} for x in domains}\n",
    "tgt_lang_id = NLLB_tokenizer_tgt.lang_code_to_id[\"eng_Latn\"]\n",
    "pretrained_model = M2M100ForConditionalGeneration.from_pretrained(pretrained_model_path).eval()\n",
    "pretrained_model = BetterTransformer.transform(pretrained_model)\n",
    "pretrained_model.to(device)\n",
    "\n",
    "for which_domain in tqdm(domains):\n",
    "    sorted_input_dataset, sorted_tokenized_answer, src_sentences, tgt_sentences = set_dataset(which_domain)\n",
    "    dataloader = set_dataloader(sorted_input_dataset, NLLB_tokenizer_src)\n",
    "\n",
    "    # translate!!\n",
    "    translated_sentences, el2n_list, entropy_list, outputs_offset_mappings \\\n",
    "        = translate(pretrained_model, dataloader, sorted_tokenized_answer, tgt_lang_id)\n",
    "\n",
    "    el2n_scores = [x.mean().item() for x in el2n_list]\n",
    "    entropy_scores = [x.mean().item() for x in entropy_list]\n",
    "    entropy_for_NE = [x for x in entropy_list]\n",
    "    translated_dict[which_domain] = {\n",
    "        'translated':translated_sentences, 'output_offset_mappings':outputs_offset_mappings,\n",
    "        'el2n':el2n_scores, 'entropy':entropy_scores, 'entropy_for_NE':entropy_for_NE,\n",
    "        'src':src_sentences, 'tgt':tgt_sentences}\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "del pretrained_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "save function & save entropy + el2n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bdab7ddcdc470e8fc274f44004890f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496a7b63eb5a4dddb4e30120f9779458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ff0667667c4612bc2c501b7e6a1cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babffc730bbe4e8ab3eaee84a4fea125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a9dbebcc9a49eb8837769221b21d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe4b79d48f94ae48bc57c0fb354dc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7247ea11bc4b069138eb84f1ccebf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99efe992a35b4b7abd6c954f187f1835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save(which_domain, which_method):\n",
    "    saving_dict = {'translated':translated_dict[which_domain]['translated'],\n",
    "                   which_method:translated_dict[which_domain][which_method],\n",
    "                   'src':translated_dict[which_domain]['src'],\n",
    "                   'tgt':translated_dict[which_domain]['tgt']\n",
    "                   }\n",
    "    saving_dataset_obj = Dataset.from_dict(saving_dict)\n",
    "    saving_dataset_obj.save_to_disk(f'{output_path}/{which_domain}/{which_method}')\n",
    "\n",
    "for which_domain in domains:\n",
    "    save(which_domain, 'el2n')\n",
    "    save(which_domain, 'entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NER_based_methods\n",
    "define NER function and other utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def NER(EN_NER_pipeline, which_domain):\n",
    "    translated_dataset = Dataset.from_dict({'tgt': translated_dict[which_domain]['tgt']})\n",
    "\n",
    "    ner_list = []\n",
    "    for out in tqdm(EN_NER_pipeline(KeyDataset(translated_dataset, \"tgt\"), batch_size=32), total=len(translated_dataset)):\n",
    "        ner_list.append(out)\n",
    "\n",
    "    ner_dict = {n: x for n, x in enumerate(ner_list)}\n",
    "    for key in ner_dict.keys():\n",
    "        if which_domain == 'medical':\n",
    "            ner_dict[key] = [[x['start'], x['end']] for x in ner_dict[key] if x['entity_group'] not in\n",
    "                             ['Age', 'Date', 'Frequency', 'Duration',\n",
    "                              'Distance', 'Mass', 'Sex', 'Lab_value', 'Time', 'Coreference']]\n",
    "        else:\n",
    "            ner_dict[key] = [[x['start'], x['end']] for x in ner_dict[key]]\n",
    "\n",
    "    return ner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check whether translated token is one of named entity composition\n",
    "def find_indices2(ref, special):\n",
    "    res = []\n",
    "\n",
    "    for s_interval in special:\n",
    "        for r_interval in ref:\n",
    "            if s_interval[1] >= r_interval[0] and s_interval[0] <= r_interval[1]:\n",
    "                res.append(s_interval)\n",
    "                break\n",
    "    return res\n",
    "\n",
    "# check offset_mapping's token(NLLB tokenized) index\n",
    "def find_indices_reverse_indices(outputs_offset_mappings, final_indices):\n",
    "    res = []\n",
    "    for ner_range in final_indices:\n",
    "        sub_list = []\n",
    "        for idx, offset in enumerate(outputs_offset_mappings):\n",
    "            if offset[1] <= ner_range[0]:  # If the offset is completely before the NER range\n",
    "                continue\n",
    "            elif offset[0] >= ner_range[1]:  # If the offset is completely after the NER range\n",
    "                break\n",
    "            else:\n",
    "                sub_list.append(idx)\n",
    "        res.append(sub_list)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "save NER-based methods\n",
    "\n",
    "'d4data/biomedical-ner-all' works in medical domain pretty well\n",
    "\n",
    "'RashidNLP/NER-Deberta' works in general domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|▊         | 33/400 [00:00<00:01, 314.28it/s]\u001b[A\n",
      " 22%|██▏       | 88/400 [00:00<00:00, 447.57it/s]\u001b[A\n",
      " 34%|███▍      | 137/400 [00:00<00:00, 466.67it/s]\u001b[A\n",
      " 48%|████▊     | 193/400 [00:00<00:00, 497.36it/s]\u001b[A\n",
      " 64%|██████▍   | 257/400 [00:00<00:00, 548.13it/s]\u001b[A\n",
      "100%|██████████| 400/400 [00:00<00:00, 588.24it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be27b0a6118476cad7281871b9cb126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4986d3677d4b4b7e98c07845701209c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 7/320 [00:00<00:04, 70.00it/s]\u001b[A\n",
      " 14%|█▍        | 45/320 [00:00<00:01, 249.41it/s]\u001b[A\n",
      " 30%|██▉       | 95/320 [00:00<00:00, 361.47it/s]\u001b[A\n",
      " 41%|████▏     | 132/320 [00:00<00:00, 342.01it/s]\u001b[A\n",
      " 60%|██████    | 193/320 [00:00<00:00, 401.15it/s]\u001b[A\n",
      "100%|██████████| 320/320 [00:00<00:00, 444.44it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7bc43ded0a4e37ba61b2f029411d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857c0388357847c1bae40a71efabb455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:04<00:04,  2.33s/it]\n",
      "  0%|          | 0/240 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|█▎        | 32/240 [00:00<00:00, 316.83it/s]\u001b[A\n",
      " 27%|██▋       | 65/240 [00:00<00:00, 277.42it/s]\u001b[A\n",
      " 54%|█████▍    | 129/240 [00:00<00:00, 378.16it/s]\u001b[A\n",
      "100%|██████████| 240/240 [00:00<00:00, 455.41it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28172e64694f44d7a2d7bf4c3b1ef5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e012df90c094747abc4d31aa82dd53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:06<00:02,  2.38s/it]\n",
      "  0%|          | 0/320 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▌         | 19/320 [00:00<00:01, 186.27it/s]\u001b[A\n",
      " 18%|█▊        | 57/320 [00:00<00:00, 297.56it/s]\u001b[A\n",
      " 30%|███       | 97/320 [00:00<00:00, 299.24it/s]\u001b[A\n",
      " 50%|█████     | 160/320 [00:00<00:00, 418.40it/s]\u001b[A\n",
      " 63%|██████▎   | 203/320 [00:00<00:00, 415.28it/s]\u001b[A\n",
      "100%|██████████| 320/320 [00:00<00:00, 450.70it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701774c51ad54ec793fc352ac3f5d2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f99a5664ff64af98a7b7936e8e69526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:09<00:00,  2.50s/it]\n"
     ]
    }
   ],
   "source": [
    "for which_domain in tqdm(domains):\n",
    "    if which_domain=='medical':\n",
    "        NER_model_path = 'd4data/biomedical-ner-all'\n",
    "        ner_model = AutoModelForTokenClassification.from_pretrained(NER_model_path).eval()\n",
    "        ner_tokenizer = AutoTokenizer.from_pretrained(NER_model_path, model_max_length=144)\n",
    "        EN_NER_pipeline = TokenClassificationPipeline(model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy=\"simple\",\n",
    "                                                      device = device)\n",
    "    else:\n",
    "        NER_model_path = 'RashidNLP/NER-Deberta'\n",
    "        ner_model = AutoModelForTokenClassification.from_pretrained(NER_model_path).eval()\n",
    "        ner_tokenizer = AutoTokenizer.from_pretrained(NER_model_path, model_max_length=144)\n",
    "        EN_NER_pipeline = TokenClassificationPipeline(model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy=\"simple\",\n",
    "                                                      device = device)\n",
    "\n",
    "    NER_predicted_dict = NER(EN_NER_pipeline, which_domain)\n",
    "    entropy_NE_mean_list = []\n",
    "    entropy_NE_list = []\n",
    "    for key in NER_predicted_dict.keys():\n",
    "        Distorted_NEs_indices = find_indices2(\n",
    "            translated_dict[which_domain]['output_offset_mappings'][key], NER_predicted_dict[key])\n",
    "        Distorted_NEs_indices = find_indices_reverse_indices(\n",
    "            translated_dict[which_domain]['output_offset_mappings'][key], Distorted_NEs_indices)\n",
    "        Distorted_NEs_indices = [j for sub in Distorted_NEs_indices for j in sub]\n",
    "        if len(Distorted_NEs_indices) == 0:\n",
    "            entropy_NE_list.append(-1)\n",
    "            entropy_NE_mean_list.append(-1)\n",
    "            continue\n",
    "\n",
    "        entropy_NE_list.append(translated_dict[which_domain]['entropy_for_NE'][key][Distorted_NEs_indices].max().item())\n",
    "        entropy_NE_mean_list.append(translated_dict[which_domain]['entropy_for_NE'][key][Distorted_NEs_indices].mean().item())\n",
    "\n",
    "    translated_dict[which_domain]['entropy_NE'] = entropy_NE_list\n",
    "    translated_dict[which_domain]['entropy_NE_mean']  = entropy_NE_mean_list\n",
    "    save(which_domain, 'entropy_NE')\n",
    "    save(which_domain, 'entropy_NE_mean')\n",
    "\n",
    "    del ner_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Embeddings_based_methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_means(model, model_path, which_domain, label_size=128, batch_size=128):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    def tokenize(row):\n",
    "        return tokenizer(row['src'], truncation=True, max_length=72) # 36+50, mean + 4 sigma\n",
    "\n",
    "    src_sentences_dataset = Dataset.from_dict({'src':translated_dict[which_domain]['src']})\n",
    "    tokenized_src_dataset = src_sentences_dataset.map(tokenize, batched=True)\n",
    "    tokenized_src_dataset = tokenized_src_dataset.remove_columns(['src', 'token_type_ids'])\n",
    "    dataloader = set_dataloader(tokenized_src_dataset, tokenizer)\n",
    "\n",
    "    embeddings_list = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(dataloader):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            embeddings, _ = model(**inputs, return_dict=False)\n",
    "            embeddings_list.append(embeddings[:,0,:].cpu())\n",
    "\n",
    "    embeddings_tensors = torch.cat(embeddings_list, axis=0)\n",
    "    kmeans = KMeans(n_clusters=label_size, random_state=42, n_init=\"auto\").fit(embeddings_tensors)\n",
    "    label_list = kmeans.labels_.tolist()\n",
    "    centers = torch.tensor(kmeans.cluster_centers_)\n",
    "\n",
    "    eu_val_list = []\n",
    "    for n, cluster_idx in enumerate(label_list):\n",
    "        dist = (embeddings_tensors[n] - centers[cluster_idx]).pow(2).sum().sqrt()\n",
    "        eu_val_list.append(dist.item())\n",
    "\n",
    "    return eu_val_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "save embedding based methods.\n",
    "\n",
    "'BM-K/KoSimCSE-roberta-multitask' works pretty well in Korean language\n",
    "\n",
    "'sentence-transformers/LaBSE' is good at multilingual sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a396d8030db492c9e55c5fa7a9e15a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[AYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      " 22%|██▏       | 11/50 [00:00<00:00, 101.85it/s]\u001b[A\n",
      " 44%|████▍     | 22/50 [00:00<00:00, 105.89it/s]\u001b[A\n",
      " 68%|██████▊   | 34/50 [00:00<00:00, 108.32it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:00<00:00, 109.41it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b094398ab7fa460f836581c39999a445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[AYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      " 22%|██▏       | 11/50 [00:00<00:00, 102.80it/s]\u001b[A\n",
      " 46%|████▌     | 23/50 [00:00<00:00, 108.30it/s]\u001b[A\n",
      " 70%|███████   | 35/50 [00:00<00:00, 111.01it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:00<00:00, 109.89it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e4314424824f7384e2b3287772693c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d9419ec28d4363aa8f48e0df0cbca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:06<00:20,  6.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e4a012defa4b2b8177b0dcf5750080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[AYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      " 25%|██▌       | 10/40 [00:00<00:00, 96.48it/s]\u001b[A\n",
      " 55%|█████▌    | 22/40 [00:00<00:00, 104.66it/s]\u001b[A\n",
      "100%|██████████| 40/40 [00:00<00:00, 107.92it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf5830a5d8142529b4d75cfa5fbc89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[AYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      " 25%|██▌       | 10/40 [00:00<00:00, 98.04it/s]\u001b[A\n",
      " 55%|█████▌    | 22/40 [00:00<00:00, 105.91it/s]\u001b[A\n",
      "100%|██████████| 40/40 [00:00<00:00, 107.64it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acecde074c2645ee8145e852e1d1e5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354c9e1caa7b4c0d9503768d5b24c25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:13<00:13,  6.76s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc5208ff5bd4b2386eb812088d4120c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[AYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      " 37%|███▋      | 11/30 [00:00<00:00, 103.77it/s]\u001b[A\n",
      "100%|██████████| 30/30 [00:00<00:00, 109.09it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706246a5d2324301b12ba293dbd9ebf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[AYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      " 37%|███▋      | 11/30 [00:00<00:00, 104.76it/s]\u001b[A\n",
      "100%|██████████| 30/30 [00:00<00:00, 109.49it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76dd659d04744126a6a47362ed45f063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d014dde51b14459abb52c2611019e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:20<00:06,  6.74s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493f7d0fc3524e7b954a28e1b8f7af29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[AYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      " 25%|██▌       | 10/40 [00:00<00:00, 93.46it/s]\u001b[A\n",
      " 52%|█████▎    | 21/40 [00:00<00:00, 102.33it/s]\u001b[A\n",
      "100%|██████████| 40/40 [00:00<00:00, 106.95it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b52ab9c288147eb85ef9c007d049ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[AYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      " 25%|██▌       | 10/40 [00:00<00:00, 99.01it/s]\u001b[A\n",
      " 55%|█████▌    | 22/40 [00:00<00:00, 107.53it/s]\u001b[A\n",
      "100%|██████████| 40/40 [00:00<00:00, 109.29it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b4a6613b864da09e1d204b23920b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad33349d995d4f28ab3721e349fbae9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:26<00:00,  6.70s/it]\n"
     ]
    }
   ],
   "source": [
    "multilingual_model_path = 'sentence-transformers/LaBSE'\n",
    "monolingual_model_path = 'BM-K/KoSimCSE-roberta-multitask'\n",
    "\n",
    "for which_domain in tqdm(domains):\n",
    "    multilingual_embeds_model = AutoModel.from_pretrained(multilingual_model_path)\n",
    "    multilingual_embeds_model.to(device)\n",
    "    multilingual_embeds_val = k_means(multilingual_embeds_model, multilingual_model_path, which_domain)\n",
    "\n",
    "    monolingual_embeds_model = AutoModel.from_pretrained(monolingual_model_path)\n",
    "    monolingual_embeds_model.to(device)\n",
    "    monolingual_embeds_val = k_means(monolingual_embeds_model, monolingual_model_path, which_domain)\n",
    "\n",
    "    translated_dict[which_domain]['self_sup_multi'] = multilingual_embeds_val\n",
    "    translated_dict[which_domain]['self_sup_mono']  = monolingual_embeds_val\n",
    "    save(which_domain, 'self_sup_multi')\n",
    "    save(which_domain, 'self_sup_mono')\n",
    "\n",
    "    del multilingual_embeds_model\n",
    "    del monolingual_embeds_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ref-free Comet based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prune_by_comet(comet_model, which_domain):\n",
    "    dataset_for_comet = Dataset.from_dict(\n",
    "        {'src': translated_dict[which_domain]['src'], 'mt': translated_dict[which_domain]['translated'],\n",
    "         'tgt': translated_dict[which_domain]['tgt']})\n",
    "    dataset_for_comet_list = dataset_for_comet.to_list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = comet_model.predict(dataset_for_comet_list, batch_size=4, gpus=1, num_workers=0)\n",
    "\n",
    "    comet_scores = [x for x in model_output['scores']]\n",
    "    return comet_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "save comet based methods.\n",
    "\n",
    "'Unbabel/wmt23-cometkiwi-da-xl' is a recent model and used for experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015048f73b4c4aecbed9a11771fac0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.2 to v2.0.9.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file C:\\Users\\jish1\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-cometkiwi-da\\snapshots\\b3a8aea5a5fc22db68a554b92b3d96eb6ea75cc9\\checkpoints\\model.ckpt`\n",
      "Encoder model frozen.\n",
      "C:\\Users\\jish1\\PycharmProjects\\COLING-paper-project\\venv\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:165: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|██████████| 50/50 [00:01<00:00, 37.52it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae7d7127af344238c07ed1cf1cbe988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|██████████| 40/40 [00:01<00:00, 34.57it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dbbe71a9e14e598cc103e031b5e01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|██████████| 30/30 [00:00<00:00, 32.89it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8173328dc9354124b9265c54e252364b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|██████████| 40/40 [00:01<00:00, 34.97it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf4d127e84b4254b75a88bce6172305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comet_model_path = 'Unbabel/wmt22-cometkiwi-da'\n",
    "comet_model_path = download_model(comet_model_path)\n",
    "comet_model = load_from_checkpoint(comet_model_path)\n",
    "\n",
    "for which_domain in domains:\n",
    "    comet_scores = prune_by_comet(comet_model, which_domain)\n",
    "    translated_dict[which_domain]['refree_comet']  = comet_scores\n",
    "    save(which_domain, 'refree_comet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
